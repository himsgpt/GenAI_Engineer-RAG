{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65312315",
   "metadata": {},
   "source": [
    "## RAG Options (Post-chunking) in LangChain ‚Äî Categorized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b227d617",
   "metadata": {},
   "outputs": [],
   "source": [
    "### üß† Embedding Options for RAG (Post-Chunking)\n",
    "\n",
    "| Category                           | Providers / Methods                                                                 | Requires API Key | Downloads Model Locally | Notes                                                                 |\n",
    "|------------------------------------|--------------------------------------------------------------------------------------|------------------|---------------------------|-----------------------------------------------------------------------|\n",
    "| üõ∞Ô∏è Cloud-based API Providers       | `OpenAIEmbeddings`, `CohereEmbeddings`, `AzureOpenAIEmbeddings`, `VertexAIEmbeddings`, `BedrockEmbeddings` | ‚úÖ Yes           | ‚ùå No                    | Remote proprietary APIs. Fast, scalable, paid beyond free tiers.     |\n",
    "| üß† Local Inference (Downloaded)    | `HuggingFaceEmbeddings`, `InstructorEmbedding`, `transformers` (custom), `llama-cpp` | ‚ùå No            | ‚úÖ Yes                   | Fully local, private. Requires downloading models and compute.        |\n",
    "| ‚òÅÔ∏è Hosted Open-Source APIs         | `HuggingFaceInferenceAPIEmbeddings`, Together AI, Replicate (custom clients)        | ‚úÖ Yes           | ‚ùå No                    | Hosted inference of open models. Slower but avoids local setup.      |\n",
    "| ‚öôÔ∏è Local Wrappers / CLI Simplicity | `Ollama`                                                                             | ‚ùå No            | ‚úÖ Yes (on first run)    | Simplified local use. Wraps `llama.cpp`. Easy to start with.         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a6f85",
   "metadata": {},
   "source": [
    "## 1. load env "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e2014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sk-\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv('../.env')\n",
    "key = os.getenv(\"OPENAI_KEY\")\n",
    "print(key[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7954cc",
   "metadata": {},
   "source": [
    "## 2. load file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65f6eab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2025\n",
      "ROUTE LLM: L EARNING TO ROUTE LLM S WITH\n",
      "PREFERENCE DATA\n",
      "Isaac Ong‚àó1 Amjad Almahairi‚àó2 Vincent Wu1 Wei-Lin Chiang1 Tianhao Wu1\n",
      "Joseph E. Gonzalez1 M Waleed Kadous3 Ion Stoica1,2\n",
      "1UC Berkeley 2Anyscale 3Canva\n",
      "ABSTRACT\n",
      "Large language models (LLMs) excel at a wide range of tasks, but choosing the\n",
      "right model often involves balancing performance and cost. Powerful models offer\n",
      "better results but are expensive, while smaller models are more cost-effective but\n",
      "less capable. To address this trade-off, we introduce a training framework for\n",
      "learning efficient router models that dynamically select between a stronger and\n",
      "weaker LLM during inference. Our framework leverages human preference data\n",
      "and employs data augmentation techniques to enhance performance. Evaluations\n",
      "on public benchmarks show that our approach can reduce costs by over 2 times\n",
      "without sacrificing response quality. Moreover, our routers exhibit strong general-\n",
      "ization capabilities, maintaining performance even when routing between LLMs\n",
      "not included in training. This highlights the potential of our framework to deliver\n",
      "cost-effective, high-performance LLM solutions.\n",
      "1 I NTRODUCTION\n",
      "Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across\n",
      "a wide range of natural language tasks. From open-ended conversation and question answering to\n",
      "text summarization and code generation, LLMs have demonstrated an impressive level of fluency and\n",
      "understanding (Achiam et al., 2023; Bubeck et al., 2023). This rapid progress has been enabled by a\n",
      "combination of architectural innovations, such as the Transformer architecture (Vaswani et al., 2017),\n",
      "as well as scaling up data and training infrastructure (Brown et al., 2020; Radford et al., 2019).\n",
      "However, not all LLMs are created equal‚Äîthere exists wide variation in the sizes of different LLMs,\n",
      "which in turn affects the resources required to serve them. LLMs also differ in terms of the data on\n",
      "which they are trained, which in turn leads to variations in the strengths, weaknesses, and capabilities\n",
      "of different models. Broadly speaking, larger models tend to be more capable but come at a higher\n",
      "cost, while smaller models tend to be less capable but cheaper to serve.\n",
      "This heterogeneous landscape presents a dilemma in the practical deployment of LLMs. Although\n",
      "routing all user queries to the largest and most capable model ensures high-quality results, it is\n",
      "prohibitively expensive. Conversely, routing queries to smaller models can save costs‚Äîby more than\n",
      "50x (e.g., Claude-3 Haiku vs. Opus 1)‚Äîbut may result in lower quality responses, as the smaller\n",
      "model may not handle complex queries effectively.\n",
      "LLM routing (Ding et al., 2024; Hu et al., 2024) offers an effective solution by first processing each\n",
      "user query through a router, which then determines the most suitable LLM to handle the query. The\n",
      "router can direct simpler queries to smaller models and more complex ones to larger models, thereby\n",
      "balancing response quality with cost efficiency.\n",
      "Achieving optimal LLM routing‚Äîmaximizing quality within a cost constraint or minimizing cost\n",
      "for a target quality‚Äîis challenging. An ideal LLM router must (1) optimize response quality while\n",
      "invoking a single LLM per query, minimizing cost and latency as compared to multi-LLM approaches;\n",
      "(2) generalize to out-of-domain queries without needing separate routers for different domains; and\n",
      "(3) work across a broad range of LLMs without retraining, ensuring flexibility as the LLM landscape\n",
      "evolves.\n",
      "‚àóEqual contribution. Correspondence to isaacong@berkeley.edu, anm@anyscale.com.\n",
      "1Per one million output tokens: Haiku ($1.25) vs. Opus ($75)\n",
      "1\n",
      "arXiv:2406.18665v4  [cs.LG]  23 Feb 2025' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-25T01:57:29+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-25T01:57:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../llm_router.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "#2 load\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader('../llm_router.pdf')\n",
    "document = loader.load()\n",
    "print(document[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85fc621",
   "metadata": {},
   "source": [
    "## 3. chunking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "378cb16d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='Published as a conference paper at ICLR 2025\n",
      "ROUTE LLM: L EARNING TO ROUTE LLM S WITH\n",
      "PREFERENCE DATA\n",
      "Isaac Ong‚àó1 Amjad Almahairi‚àó2 Vincent Wu1 Wei-Lin Chiang1 Tianhao Wu1\n",
      "Joseph E. Gonzalez1 M Waleed Kadous3 Ion Stoica1,2' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-25T01:57:29+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-25T01:57:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../llm_router.pdf', 'total_pages': 16, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "# 3 chunk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=100)\n",
    "docs = chunk.split_documents(documents=document)\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1691551",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_no:</th>\n",
       "      <th>content</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Joseph E. Gonzalez1 M Waleed Kadous3 Ion Stoic...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>right model often involves balancing performan...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>less capable. To address this trade-off, we in...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>weaker LLM during inference. Our framework lev...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>358</td>\n",
       "      <td>optimize for performance and specify the maxim...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359</th>\n",
       "      <td>359</td>\n",
       "      <td>token ratio so that 50% of calls are routed to...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>360</td>\n",
       "      <td>Both the matrix factorization router and causa...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>361</td>\n",
       "      <td>with up to 40% fewer calls routed to GPT-4.\\nF...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>362</td>\n",
       "      <td>Published as a conference paper at ICLR 2025\\n...</td>\n",
       "      <td>{'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>363 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     chunk_no:                                            content  \\\n",
       "0            0  Published as a conference paper at ICLR 2025\\n...   \n",
       "1            1  Joseph E. Gonzalez1 M Waleed Kadous3 Ion Stoic...   \n",
       "2            2  right model often involves balancing performan...   \n",
       "3            3  less capable. To address this trade-off, we in...   \n",
       "4            4  weaker LLM during inference. Our framework lev...   \n",
       "..         ...                                                ...   \n",
       "358        358  optimize for performance and specify the maxim...   \n",
       "359        359  token ratio so that 50% of calls are routed to...   \n",
       "360        360  Both the matrix factorization router and causa...   \n",
       "361        361  with up to 40% fewer calls routed to GPT-4.\\nF...   \n",
       "362        362  Published as a conference paper at ICLR 2025\\n...   \n",
       "\n",
       "                                              metadata  \n",
       "0    {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "1    {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "2    {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "3    {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "4    {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "..                                                 ...  \n",
       "358  {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "359  {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "360  {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "361  {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "362  {'producer': 'pdfTeX-1.40.25', 'creator': 'LaT...  \n",
       "\n",
       "[363 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont = []\n",
    "import pandas as pd\n",
    "for i, doc in enumerate(docs):\n",
    "    cont.append({\n",
    "        \"chunk_no:\": i,\n",
    "        \"content\": doc.page_content, \n",
    "        \"metadata\": doc.metadata\n",
    "    })\n",
    "pd.DataFrame(cont)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b898d230",
   "metadata": {},
   "source": [
    "## 4. Embedding and vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d506eb99",
   "metadata": {},
   "source": [
    "### Option 1 (provider but not free of cost, so not working)\n",
    "#### install tiktoken, openai "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b1d14629",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FAISS\n\u001b[0;32m      4\u001b[0m embedding \u001b[38;5;241m=\u001b[39m OpenAIEmbeddings(openai_api_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[1;32m----> 5\u001b[0m vectordb \u001b[38;5;241m=\u001b[39m \u001b[43mFAISS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m vectordb\u001b[38;5;241m.\u001b[39msave_local(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaissdb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\langchain_core\\vectorstores\\base.py:847\u001b[0m, in \u001b[0;36mVectorStore.from_documents\u001b[1;34m(cls, documents, embedding, **kwargs)\u001b[0m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[0;32m    845\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ids\n\u001b[1;32m--> 847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_texts(texts, embedding, metadatas\u001b[38;5;241m=\u001b[39mmetadatas, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[0m, in \u001b[0;36mFAISS.from_texts\u001b[1;34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_texts\u001b[39m(\n\u001b[0;32m   1018\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m   1024\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m FAISS:\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[0;32m   1026\u001b[0m \n\u001b[0;32m   1027\u001b[0m \u001b[38;5;124;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;124;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1043\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__from(\n\u001b[0;32m   1045\u001b[0m         texts,\n\u001b[0;32m   1046\u001b[0m         embeddings,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1051\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:671\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    670\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 671\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:497\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    495\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n\u001b[1;32m--> 497\u001b[0m     response \u001b[38;5;241m=\u001b[39m embed_with_retry(\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    499\u001b[0m         \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39mtokens[i : i \u001b[38;5;241m+\u001b[39m _chunk_size],\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_invocation_params,\n\u001b[0;32m    501\u001b[0m     )\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    503\u001b[0m         response \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mdict()\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\langchain_community\\embeddings\\openai.py:120\u001b[0m, in \u001b[0;36membed_with_retry\u001b[1;34m(embeddings, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the embedding call.\"\"\"\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mcreate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    121\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(embeddings)\n\u001b[0;32m    123\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\openai\\resources\\embeddings.py:128\u001b[0m, in \u001b[0;36mEmbeddings.create\u001b[1;34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    122\u001b[0m             embedding\u001b[38;5;241m.\u001b[39membedding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(  \u001b[38;5;66;03m# type: ignore[no-untyped-call]\u001b[39;00m\n\u001b[0;32m    123\u001b[0m                 base64\u001b[38;5;241m.\u001b[39mb64decode(data), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m             )\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/embeddings\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbeddingCreateParams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpost_parser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCreateEmbeddingResponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\openai\\_base_client.py:1239\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1227\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1234\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1235\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1236\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1237\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1238\u001b[0m     )\n\u001b[1;32m-> 1239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\openai\\_base_client.py:1034\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1031\u001b[0m             err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1033\u001b[0m         log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1034\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1038\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcould not resolve response (should never happen)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
     ]
    }
   ],
   "source": [
    "#4 embedding + vectorize \n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "embedding = OpenAIEmbeddings(openai_api_key=key)\n",
    "vectordb = FAISS.from_documents(docs, embedding)\n",
    "vectordb.save_local(\"faissdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff446ba",
   "metadata": {},
   "source": [
    "### Option 2 (huggingface)\n",
    "#### install pip install langchain faiss-cpu, pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ec61e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HIMANSHU\\AppData\\Local\\Temp\\ipykernel_12456\\1466544340.py:3: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  embedding = HuggingFaceBgeEmbeddings()\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceBgeEmbeddings\n",
    "# embedding = HuggingFaceBgeEmbeddings(\"BAAI/bge-small-en-v1.5\", model_kwargs={'device':'cpu'})\n",
    "embedding = HuggingFaceBgeEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e0ccb24f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing vector database at faissdb\n",
      "Saved new FAISS vector database at faissdb\n",
      "Total vectors stored: 363\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorpath = \"faissdb\"\n",
    "\n",
    "# Step 1: Delete existing folder if it exists\n",
    "if os.path.exists(vectorpath):\n",
    "    shutil.rmtree(vectorpath)\n",
    "    print(f\"Deleted existing vector database at {vectorpath}\")\n",
    "\n",
    "# Step 2: Create and save new FAISS db\n",
    "vectordb = FAISS.from_documents(docs, embedding)\n",
    "vectordb.save_local(vectorpath)\n",
    "\n",
    "# Step 3: Confirm save\n",
    "print(f\"Saved new FAISS vector database at {vectorpath}\")\n",
    "print(f\"Total vectors stored: {vectordb.index.ntotal}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8503b47",
   "metadata": {},
   "source": [
    "## 5. retrieve top k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04ee72ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363\n",
      "In this work, we introduce a principled framework for learning LLM routers from preference data.\n",
      "Our approach involves routing between two classes of models: (1) strong models, which provide\n",
      "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-25T01:57:29+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-25T01:57:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../llm_router.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}\n"
     ]
    }
   ],
   "source": [
    "# load vectordb and retrieve top k\n",
    "vectorpath = \"faissdb\"\n",
    "vectordb = FAISS.load_local(vectorpath,embedding, allow_dangerous_deserialization=True)\n",
    "print(vectordb.index.ntotal)\n",
    "retriever = vectordb.as_retriever(search_kwargs={\n",
    "                                                'k': 1\n",
    "})\n",
    "question = \"give me llm router algorithms?\"\n",
    "results = retriever.get_relevant_documents(question, filter={'keywords':''})\n",
    "print(results[0].page_content)\n",
    "print(results[0].metadata)\n",
    "\n",
    "# or \n",
    "# result = vectordb.similarity_search(query=question, k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc43d0",
   "metadata": {},
   "source": [
    "## 6. Use LLM for chains"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c366889",
   "metadata": {},
   "source": [
    "### openAI/groq will not work so use option - 1 huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dd3eedda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "## llms\n",
    "# from langchain.llms import groq\n",
    "# cached at C:\\Users\\HIMANSHU\\.cache\\huggingface\\hub\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "text_pipeline = pipeline(\n",
    "                    # \"text-generation\",\n",
    "                    \"text2text-generation\",\n",
    "                    model=\"google/flan-t5-small\",\n",
    "                    # model=\"google/flan-t5-base\",\n",
    "                     max_length=1024,\n",
    "                     temperature=0.5,\n",
    "                     device=-1)\n",
    "llm = HuggingFacePipeline(pipeline=text_pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "955df4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} template=\"You are a RAG expert. Use the following context to answer the question at the end. If you don't know the answer, just say you don't know. Don't try to make up an answer.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nHelpful Answer:\"\n"
     ]
    }
   ],
   "source": [
    "## prompt template\n",
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"You are a RAG expert. Use the following context to answer the question at the end. If you don't know the answer, just say you don't know. Don't try to make up an answer.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['context', 'question'], template=prompt_template)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "98e22102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\HIMANSHU\\anaconda3\\envs\\vecdb\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'give me llm router algorithms?', 'result': '(ii)', 'source_documents': [Document(id='f0d9df1b-5dc9-4286-a1cc-51a3cb964ae4', metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-02-25T01:57:29+00:00', 'author': '', 'keywords': '', 'moddate': '2025-02-25T01:57:29+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '../llm_router.pdf', 'total_pages': 16, 'page': 1, 'page_label': '2'}, page_content='In this work, we introduce a principled framework for learning LLM routers from preference data.\\nOur approach involves routing between two classes of models: (1) strong models, which provide')]}\n"
     ]
    }
   ],
   "source": [
    "## chains\n",
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                    retriever=retriever, \n",
    "                    chain_type='stuff',\n",
    "                    chain_type_kwargs ={'prompt':prompt},\n",
    "                    return_source_documents=True)\n",
    "question = \"give me llm router algorithms?\"\n",
    "result = chain(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe04fdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give me llm router algorithms?\n",
      "(ii)\n"
     ]
    }
   ],
   "source": [
    "print(result['query'])\n",
    "print(result['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882b8125",
   "metadata": {},
   "source": [
    "## debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fa8a25d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "In this work, we introduce a principled framework for learning LLM routers from preference data.\n",
      "Our approach involves routing between two classes of models: (1) strong models, which provide\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, doc in enumerate(result['source_documents']):\n",
    "    print(f\"Document {idx+1}:\\n{doc.page_content}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711422a0",
   "metadata": {},
   "source": [
    "## Option 2 llama ccp\n",
    "#### pip install llama-cpp-python (No C++ Compiler\tInstall Visual C++ Build Tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17a80a52",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LlamaCpp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mllms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m llamacpp\n\u001b[1;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaCpp\u001b[49m(\n\u001b[0;32m      3\u001b[0m     model_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath/to/your/model.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[0;32m      5\u001b[0m     max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[0;32m      6\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4096\u001b[39m,    \u001b[38;5;66;03m# set according to model capability\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# n_gpu_layers=30,  # optional: speed up if you have GPU\u001b[39;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LlamaCpp' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.llms import llamacpp\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"path/to/your/model.gguf\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=1024,\n",
    "    n_ctx=4096,    # set according to model capability\n",
    "    # n_gpu_layers=30,  # optional: speed up if you have GPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef307de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## chains\n",
    "from langchain.chains import RetrievalQA\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                    retriever=retriever, \n",
    "                    chain_type='stuff',\n",
    "                    chain_type_kwargs ={'prompt':prompt},\n",
    "                    return_source_documents=True)\n",
    "question = \"give me llm router algorithms?\"\n",
    "result = chain(question)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e0d9c2",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae3af05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "import bs4\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def initialize_llm(env_file: str = None):\n",
    "\n",
    "    if env_file:\n",
    "        load_dotenv(env_file)\n",
    "    else:\n",
    "        load_dotenv()\n",
    "\n",
    "    os.environ['USER_AGENT'] = 'myagent'\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "        openai_api_version=os.environ[\"AZURE_OPENAI_API_VERSION\"],\n",
    "    )\n",
    "\n",
    "    embeddings = AzureOpenAIEmbeddings(\n",
    "        model=\"text-embedding-3-large\",\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT_NAME\"],\n",
    "        openai_api_version=os.environ[\"AZURE_OPENAI_EMBEDDINGS_API_VERSION\"])\n",
    "    \n",
    "    return llm, embeddings\n",
    "\n",
    "def build_rag_pipeline(llm: AzureChatOpenAI, azure_embeddings: AzureOpenAIEmbeddings, documents: list):\n",
    "    # Load, chunk and index the contents of the blog.\n",
    "    loader = WebBaseLoader(\n",
    "        web_paths=documents,\n",
    "        bs_kwargs=dict(\n",
    "            parse_only=bs4.SoupStrainer(\n",
    "                class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    docs = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(docs)\n",
    "\n",
    "    vectorstore = Chroma.from_documents(documents=splits, embedding=azure_embeddings)\n",
    "\n",
    "    # Retrieve and generate using the relevant snippets of the blog.\n",
    "    retriever = vectorstore.as_retriever()\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    return rag_chain\n",
    "\n",
    "def query_rag_pipeline(rag_chain, query_text: str):\n",
    "    result = rag_chain.invoke(query_text)\n",
    "    return result\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    llm, azure_embeddings = initialize_llm(\"azure.env\")\n",
    "    documents = [\n",
    "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\"\n",
    "    ]\n",
    "\n",
    "    rag_chain = build_rag_pipeline(llm, azure_embeddings, documents)\n",
    "    query = \"What is Task decomposition?\"\n",
    "    answer = query_rag_pipeline(rag_chain, query)\n",
    "    print(f\"Question: {query}\\nAnswer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752fe220",
   "metadata": {},
   "source": [
    "## Hybrid (BM25 retriver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "# from langchain.llms import Groq\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "\n",
    "# === Load .env ===\n",
    "load_dotenv()\n",
    "\n",
    "# === Load and Chunk PDF ===\n",
    "loader = PyPDFLoader(\"your_file.pdf\")\n",
    "pages = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "documents = splitter.split_documents(pages)\n",
    "\n",
    "# === Add Metadata (e.g. page number) ===\n",
    "for i, doc in enumerate(documents):\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "\n",
    "# === Azure Embeddings ===\n",
    "embedding = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "    openai_api_type=\"azure\",\n",
    "    openai_api_version=os.getenv(\"OPENAI_API_VERSION\"),\n",
    "    deployment=os.getenv(\"AZURE_EMBEDDING_DEPLOYMENT\")\n",
    ")\n",
    "\n",
    "# === FAISS: persist vectorstore ===\n",
    "index_path = \"faiss_index\"\n",
    "if os.path.exists(index_path):\n",
    "    vectorstore = FAISS.load_local(index_path, embedding)\n",
    "else:\n",
    "    vectorstore = FAISS.from_documents(documents, embedding)\n",
    "    vectorstore.save_local(index_path)\n",
    "\n",
    "faiss_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# === BM25 retriever (keyword-based) ===\n",
    "bm25_retriever = BM25Retriever.from_documents(documents)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# === Hybrid Ensemble Retriever ===\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[faiss_retriever, bm25_retriever],\n",
    "    weights=[0.6, 0.4]  # Tune based on performance\n",
    ")\n",
    "\n",
    "# === Groq LLM ===\n",
    "llm = Groq(\n",
    "    api_key=os.getenv(\"GROQ_API_KEY\"),\n",
    "    model=os.getenv(\"GROQ_MODEL\"),\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# === Advanced Prompt Template ===\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant helping summarize and answer from document context.\n",
    "Use the following chunks to answer the question, and cite source chunk IDs when relevant.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer with sources at the end like: (Source: chunk_id 3, 5)\n",
    "\"\"\")\n",
    "\n",
    "# === Retrieval QA with Sources ===\n",
    "qa_chain = RetrievalQAWithSourcesChain.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": prompt_template}\n",
    ")\n",
    "\n",
    "# === Ask Question ===\n",
    "query = \"What are the main takeaways from the document?\"\n",
    "result = qa_chain(query)\n",
    "\n",
    "print(\"\\nAnswer:\\n\", result[\"answer\"])\n",
    "print(\"\\nSources:\\n\", result.get(\"sources\"))\n",
    "\n",
    "# === Optional: Print full text of source docs ===\n",
    "print(\"\\nTop Retrieved Chunks:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(f\"Chunk ID: {doc.metadata.get('chunk_id')}\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cc3168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"H:/Resume/xgboost_scale.pdf\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8efe8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://github.com/himsgpt\")\n",
    "docs2 = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accc584e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'H:/Resume/xgboost_scale.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}, page_content='XGBoost: A Scalable Tree Boosting System\\nTianqi Chen\\nUniversity of Washington\\ntqchen@cs.washington.edu\\nCarlos Guestrin\\nUniversity of Washington\\nguestrin@cs.washington.edu\\nABSTRACT\\nTree boosting is a highly eÔ¨Äective and widely used machine\\nlearning method. In this paper, we describe a scalable end-\\nto-end tree boosting system called XGBoost, which is used\\nwidely by data scientists to achieve state-of-the-art results\\non many machine learning challenges. We propose a novel\\nsparsity-aware algorithm for sparse data and weighted quan-\\ntile sketch for approximate tree learning. More importantly,\\nwe provide insights on cache access patterns, data compres-\\nsion and sharding to build a scalable tree boosting system.\\nBy combining these insights, XGBoost scales beyond billions\\nof examples using far fewer resources than existing systems.\\nKeywords\\nLarge-scale Machine Learning\\n1. INTRODUCTION')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "textsplitter = RecursiveCharacterTextSplitter(chunk_size=900, chunk_overlap=150, separators=[\"\\n\\n\", \"\\n\", \".\", \" \"],)\n",
    "chunks = textsplitter.split_documents(documents)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1031ba64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'producer': 'pdfTeX-1.40.12',\n",
       " 'creator': 'LaTeX with hyperref package',\n",
       " 'creationdate': '2016-06-14T01:29:40+00:00',\n",
       " 'author': '',\n",
       " 'keywords': '',\n",
       " 'moddate': '2016-06-14T01:29:40+00:00',\n",
       " 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1',\n",
       " 'subject': '',\n",
       " 'title': '',\n",
       " 'trapped': '/False',\n",
       " 'source': 'H:/Resume/xgboost_scale.pdf',\n",
       " 'total_pages': 13,\n",
       " 'page': 0,\n",
       " 'page_label': '1'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chunks[0].page_content\n",
    "chunks[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af67f827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://github.com/himsgpt',\n",
       " 'title': 'himsgpt (Himanshu Gupta) ¬∑ GitHub',\n",
       " 'description': 'With 8+ years of experience in the Data Science & Products, Himanshu specializes in Fraud and Auth modeling, Generative AI product development, ML modeling - himsgpt',\n",
       " 'language': 'en'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks2 = textsplitter.split_documents(docs2)\n",
    "chunks2[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab90316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg length: 832.33\n"
     ]
    }
   ],
   "source": [
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "print(f\"Avg length: {sum(chunk_lengths) / len(chunk_lengths):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce4c5441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Chunk 1 ---\n",
      "XGBoost: A Scalable Tree Boosting System\n",
      "Tianqi Chen\n",
      "University of Washington\n",
      "tqchen@cs.washington.edu\n",
      "Carlos Guestrin\n",
      "University of Washington\n",
      "guestrin@cs.washington.edu\n",
      "ABSTRACT\n",
      "Tree boosting is a highly eÔ¨Äective and widely used machine\n",
      "learning method. In this paper, we describe a scalable end-\n",
      "to-end tree boosting system called XGBoost, which is used\n",
      "widely by data scientists to achieve state-of-the-art results\n",
      "on many machine learning challenges. We propose a novel\n",
      "sparsity-aware algorithm for sparse data and weighted quan-\n",
      "tile sketch for approximate tree learning. More importantly,\n",
      "we provide insights on cache access patterns, data compres-\n",
      "sion and sharding to build a scalable tree boosting system.\n",
      "By combining these insights, XGBoost scales beyond billions\n",
      "of examples using far fewer resources than existing systems.\n",
      "Keywords\n",
      "Large-scale Machine Learning\n",
      "1. INTRODUCTION\n",
      "\n",
      "[Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'H:/Resume/xgboost_scale.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}]\n",
      "\n",
      "--- Chunk 2 ---\n",
      "of examples using far fewer resources than existing systems.\n",
      "Keywords\n",
      "Large-scale Machine Learning\n",
      "1. INTRODUCTION\n",
      "Machine learning and data-driven approaches are becom-\n",
      "ing very important in many areas. Smart spam classiÔ¨Åers\n",
      "protect our email by learning from massive amounts of spam\n",
      "data and user feedback; advertising systems learn to match\n",
      "the right ads with the right context; fraud detection systems\n",
      "protect banks from malicious attackers; anomaly event de-\n",
      "tection systems help experimental physicists to Ô¨Ånd events\n",
      "that lead to new physics. There are two important factors\n",
      "that drive these successful applications: usage of eÔ¨Äective\n",
      "(statistical) models that capture the complex data depen-\n",
      "dencies and scalable learning systems that learn the model\n",
      "of interest from large datasets.\n",
      "Among the machine learning methods used in practice,\n",
      "\n",
      "[Metadata: {'producer': 'pdfTeX-1.40.12', 'creator': 'LaTeX with hyperref package', 'creationdate': '2016-06-14T01:29:40+00:00', 'author': '', 'keywords': '', 'moddate': '2016-06-14T01:29:40+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.1415926-2.3-1.40.12 (TeX Live 2011) kpathsea version 6.0.1', 'subject': '', 'title': '', 'trapped': '/False', 'source': 'H:/Resume/xgboost_scale.pdf', 'total_pages': 13, 'page': 0, 'page_label': '1'}]\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks[:2]):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\")\n",
    "    print(chunk.page_content)\n",
    "    print(f\"\\n[Metadata: {chunk.metadata}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vecdb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

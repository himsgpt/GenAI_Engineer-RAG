{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPb6C7qhj58Vtue80mskZqF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/himsgpt/GenAI_Engineer-RAG/blob/main/RAG_Evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "kFInfcCp_Wku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "550b4dd1-7f9f-4e32-b4e7-e570498ffb11"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama_cpp_python\n",
            "  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (4.13.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama_cpp_python)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama_cpp_python) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama_cpp_python) (3.0.2)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: llama_cpp_python\n",
            "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4067722 sha256=6d6bd1124808440bddccc42f272246f0c012fda09834533c6fc48e5d6f56bfc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n",
            "Successfully built llama_cpp_python\n",
            "Installing collected packages: diskcache, llama_cpp_python\n",
            "Successfully installed diskcache-5.6.3 llama_cpp_python-0.3.9\n"
          ]
        }
      ],
      "source": [
        "# !pip install dotenv\n",
        "# !pip install langchain_community\n",
        "# !pip install faiss-cpu\n",
        "# ! pip install llama_cpp_python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "import os\n",
        "load_dotenv()\n",
        "key = os.getenv('KEY')"
      ],
      "metadata": {
        "id": "nuBwd_2g8Omc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load--> chunk --> embedd --> retrieve --> llm + prompt--> generate"
      ],
      "metadata": {
        "id": "1yLzum2E8Z7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import WebBaseLoader\n",
        "# loader = PyPDFLoader()\n",
        "loader = WebBaseLoader('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "id": "HDxxC5pW85nF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d45543e6-b0ad-4861-89c5-b27063448c8b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_docs(documents):\n",
        "    for doc in documents:\n",
        "        # Step 1: Access the page_content string\n",
        "        text = doc.page_content\n",
        "\n",
        "        # Step 2: Remove excessive newlines and strip lines\n",
        "        text = re.sub(r'\\n\\s*\\n+', '\\n\\n', text)  # preserve paragraph breaks\n",
        "        text = \"\\n\".join(line.strip() for line in text.splitlines())\n",
        "\n",
        "        # Step 3: Save the cleaned text back\n",
        "        doc.page_content = text\n",
        "\n",
        "    return documents\n",
        "\n",
        "cleaned_docs = clean_docs(documents)"
      ],
      "metadata": {
        "id": "0Be1iu8PI3kn"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=100)\n",
        "docs = splitter.split_documents(cleaned_docs)\n",
        "print(docs[0].page_content.strip())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "dYEoE9eV_fg1",
        "outputId": "06b904ed-cd0b-4dde-f561-8cfe73ea74f8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing - Wikipedia\n",
            "\n",
            "Jump to content\n",
            "\n",
            "Main menu\n",
            "\n",
            "Main menu\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "Navigation\n",
            "\n",
            "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
            "\n",
            "Contribute\n",
            "\n",
            "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
            "\n",
            "Search\n",
            "\n",
            "Search\n",
            "\n",
            "Appearance\n",
            "\n",
            "Donate\n",
            "\n",
            "Create account\n",
            "\n",
            "Log in\n",
            "\n",
            "Personal tools\n",
            "\n",
            "Donate Create account Log in\n",
            "\n",
            "Pages for logged out editors learn more\n",
            "\n",
            "ContributionsTalk\n",
            "\n",
            "Contents\n",
            "move to sidebar\n",
            "hide\n",
            "\n",
            "(Top)\n",
            "\n",
            "1\n",
            "History\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "embeddings = HuggingFaceEmbeddings()\n",
        "from langchain.vectorstores import FAISS\n",
        "vecdb = FAISS.from_documents(docs, embeddings)\n",
        "print(vecdb.index.ntotal)"
      ],
      "metadata": {
        "id": "Xa_LURFMAJXF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "febaead1-8f98-4f59-e78f-27c1a27e144d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-cc0ee87db8f5>:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embeddings = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "149\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vecdb.as_retriever(kwargs= {'k':3})\n",
        "query = \"what is the page talking about?\"\n",
        "result = retriever.get_relevant_documents(query)\n",
        "print(result[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k2cYA-xWDLlv",
        "outputId": "c8f988cc-b4a5-46a8-f828-6d5d39422b9d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "articles needing additional referencesAll articles with unsourced statementsArticles with unsourced statements from May 2024Commons category link from Wikidata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import LlamaCpp\n",
        "# !wget https://huggingface.co/TheBloke/phi-2-GGUF/resolve/main/phi-2.Q4_K_M.gguf\n",
        "\n",
        "llm = LlamaCpp(\n",
        "    model_path=\"phi-2.Q4_K_M.gguf\",\n",
        "    temperature=0.3,\n",
        "    max_tokens=200,\n",
        "    n_ctx=2048,\n",
        "    n_threads=4,\n",
        "    n_gpu_layers=0  # CPU-only\n",
        ")"
      ],
      "metadata": {
        "id": "8PMiH7amE5My",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "c356a9da-9f2f-4d4f-c849-b26dfa796076"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 20 key-value pairs and 325 tensors from phi-2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = phi2\n",
            "llama_model_loader: - kv   1:                               general.name str              = Phi2\n",
            "llama_model_loader: - kv   2:                        phi2.context_length u32              = 2048\n",
            "llama_model_loader: - kv   3:                      phi2.embedding_length u32              = 2560\n",
            "llama_model_loader: - kv   4:                   phi2.feed_forward_length u32              = 10240\n",
            "llama_model_loader: - kv   5:                           phi2.block_count u32              = 32\n",
            "llama_model_loader: - kv   6:                  phi2.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   7:               phi2.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   8:          phi2.attention.layer_norm_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv   9:                  phi2.rope.dimension_count u32              = 32\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  11:               tokenizer.ggml.add_bos_token bool             = false\n",
            "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,51200]   = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,51200]   = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  15:                      tokenizer.ggml.merges arr[str,50000]   = [\"Ġ t\", \"Ġ a\", \"h e\", \"i n\", \"r e\",...\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 50256\n",
            "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 50256\n",
            "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:  195 tensors\n",
            "llama_model_loader: - type q4_K:   81 tensors\n",
            "llama_model_loader: - type q5_K:   32 tensors\n",
            "llama_model_loader: - type q6_K:   17 tensors\n",
            "print_info: file format = GGUF V3 (latest)\n",
            "print_info: file type   = Q4_K - Medium\n",
            "print_info: file size   = 1.66 GiB (5.14 BPW) \n",
            "load: missing pre-tokenizer type, using: 'default'\n",
            "load:                                             \n",
            "load: ************************************        \n",
            "load: GENERATION QUALITY WILL BE DEGRADED!        \n",
            "load: CONSIDER REGENERATING THE MODEL             \n",
            "load: ************************************        \n",
            "load:                                             \n",
            "init_tokenizer: initializing tokenizer for type 2\n",
            "load: special tokens cache size = 944\n",
            "load: token to piece cache size = 0.3151 MB\n",
            "print_info: arch             = phi2\n",
            "print_info: vocab_only       = 0\n",
            "print_info: n_ctx_train      = 2048\n",
            "print_info: n_embd           = 2560\n",
            "print_info: n_layer          = 32\n",
            "print_info: n_head           = 32\n",
            "print_info: n_head_kv        = 32\n",
            "print_info: n_rot            = 32\n",
            "print_info: n_swa            = 0\n",
            "print_info: n_swa_pattern    = 1\n",
            "print_info: n_embd_head_k    = 80\n",
            "print_info: n_embd_head_v    = 80\n",
            "print_info: n_gqa            = 1\n",
            "print_info: n_embd_k_gqa     = 2560\n",
            "print_info: n_embd_v_gqa     = 2560\n",
            "print_info: f_norm_eps       = 1.0e-05\n",
            "print_info: f_norm_rms_eps   = 0.0e+00\n",
            "print_info: f_clamp_kqv      = 0.0e+00\n",
            "print_info: f_max_alibi_bias = 0.0e+00\n",
            "print_info: f_logit_scale    = 0.0e+00\n",
            "print_info: f_attn_scale     = 0.0e+00\n",
            "print_info: n_ff             = 10240\n",
            "print_info: n_expert         = 0\n",
            "print_info: n_expert_used    = 0\n",
            "print_info: causal attn      = 1\n",
            "print_info: pooling type     = 0\n",
            "print_info: rope type        = 2\n",
            "print_info: rope scaling     = linear\n",
            "print_info: freq_base_train  = 10000.0\n",
            "print_info: freq_scale_train = 1\n",
            "print_info: n_ctx_orig_yarn  = 2048\n",
            "print_info: rope_finetuned   = unknown\n",
            "print_info: ssm_d_conv       = 0\n",
            "print_info: ssm_d_inner      = 0\n",
            "print_info: ssm_d_state      = 0\n",
            "print_info: ssm_dt_rank      = 0\n",
            "print_info: ssm_dt_b_c_rms   = 0\n",
            "print_info: model type       = 3B\n",
            "print_info: model params     = 2.78 B\n",
            "print_info: general.name     = Phi2\n",
            "print_info: vocab type       = BPE\n",
            "print_info: n_vocab          = 51200\n",
            "print_info: n_merges         = 50000\n",
            "print_info: BOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOS token        = 50256 '<|endoftext|>'\n",
            "print_info: EOT token        = 50256 '<|endoftext|>'\n",
            "print_info: UNK token        = 50256 '<|endoftext|>'\n",
            "print_info: LF token         = 198 'Ċ'\n",
            "print_info: EOG token        = 50256 '<|endoftext|>'\n",
            "print_info: max token length = 256\n",
            "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
            "load_tensors: layer   0 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   2 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   4 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   6 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   8 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  10 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  12 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  14 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  16 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  18 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  20 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  22 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  24 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  27 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  28 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  29 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  30 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  31 assigned to device CPU, is_swa = 0\n",
            "load_tensors: layer  32 assigned to device CPU, is_swa = 0\n",
            "load_tensors: tensor 'token_embd.weight' (q4_K) (and 244 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
            "load_tensors:  CPU_AARCH64 model buffer size =   787.50 MiB\n",
            "load_tensors:   CPU_Mapped model buffer size =  1704.63 MiB\n",
            "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.3.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.6.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.13.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.16.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.19.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.21.ffn_down.weight with q4_K_8x8\n",
            "repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.24.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.26.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.26.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.27.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.27.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.28.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.28.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.29.ffn_down.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.29.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.30.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.30.ffn_up.weight with q4_K_8x8\n",
            ".repack: repack tensor blk.31.attn_output.weight with q4_K_8x8\n",
            "repack: repack tensor blk.31.ffn_up.weight with q4_K_8x8\n",
            "..............................................\n",
            "llama_context: constructing llama_context\n",
            "llama_context: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
            "llama_context: n_seq_max     = 1\n",
            "llama_context: n_ctx         = 2048\n",
            "llama_context: n_ctx_per_seq = 2048\n",
            "llama_context: n_batch       = 64\n",
            "llama_context: n_ubatch      = 8\n",
            "llama_context: causal_attn   = 1\n",
            "llama_context: flash_attn    = 0\n",
            "llama_context: freq_base     = 10000.0\n",
            "llama_context: freq_scale    = 1\n",
            "set_abort_callback: call\n",
            "llama_context:        CPU  output buffer size =     0.20 MiB\n",
            "create_memory: n_ctx = 2048 (padded)\n",
            "llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1, padding = 32\n",
            "llama_kv_cache_unified: layer   0: dev = CPU\n",
            "llama_kv_cache_unified: layer   1: dev = CPU\n",
            "llama_kv_cache_unified: layer   2: dev = CPU\n",
            "llama_kv_cache_unified: layer   3: dev = CPU\n",
            "llama_kv_cache_unified: layer   4: dev = CPU\n",
            "llama_kv_cache_unified: layer   5: dev = CPU\n",
            "llama_kv_cache_unified: layer   6: dev = CPU\n",
            "llama_kv_cache_unified: layer   7: dev = CPU\n",
            "llama_kv_cache_unified: layer   8: dev = CPU\n",
            "llama_kv_cache_unified: layer   9: dev = CPU\n",
            "llama_kv_cache_unified: layer  10: dev = CPU\n",
            "llama_kv_cache_unified: layer  11: dev = CPU\n",
            "llama_kv_cache_unified: layer  12: dev = CPU\n",
            "llama_kv_cache_unified: layer  13: dev = CPU\n",
            "llama_kv_cache_unified: layer  14: dev = CPU\n",
            "llama_kv_cache_unified: layer  15: dev = CPU\n",
            "llama_kv_cache_unified: layer  16: dev = CPU\n",
            "llama_kv_cache_unified: layer  17: dev = CPU\n",
            "llama_kv_cache_unified: layer  18: dev = CPU\n",
            "llama_kv_cache_unified: layer  19: dev = CPU\n",
            "llama_kv_cache_unified: layer  20: dev = CPU\n",
            "llama_kv_cache_unified: layer  21: dev = CPU\n",
            "llama_kv_cache_unified: layer  22: dev = CPU\n",
            "llama_kv_cache_unified: layer  23: dev = CPU\n",
            "llama_kv_cache_unified: layer  24: dev = CPU\n",
            "llama_kv_cache_unified: layer  25: dev = CPU\n",
            "llama_kv_cache_unified: layer  26: dev = CPU\n",
            "llama_kv_cache_unified: layer  27: dev = CPU\n",
            "llama_kv_cache_unified: layer  28: dev = CPU\n",
            "llama_kv_cache_unified: layer  29: dev = CPU\n",
            "llama_kv_cache_unified: layer  30: dev = CPU\n",
            "llama_kv_cache_unified: layer  31: dev = CPU\n",
            "llama_kv_cache_unified:        CPU KV buffer size =   640.00 MiB\n",
            "llama_kv_cache_unified: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
            "llama_context: enumerating backends\n",
            "llama_context: backend_ptrs.size() = 1\n",
            "llama_context: max_nodes = 65536\n",
            "llama_context: worst-case: n_tokens = 8, n_seqs = 1, n_outputs = 0\n",
            "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n",
            "llama_context: reserving graph for n_tokens = 8, n_seqs = 1\n",
            "llama_context:        CPU compute buffer size =     3.05 MiB\n",
            "llama_context: graph nodes  = 1289\n",
            "llama_context: graph splits = 1\n",
            "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '50256', 'tokenizer.ggml.eos_token_id': '50256', 'tokenizer.ggml.bos_token_id': '50256', 'general.architecture': 'phi2', 'general.name': 'Phi2', 'phi2.context_length': '2048', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'tokenizer.ggml.add_bos_token': 'false', 'phi2.embedding_length': '2560', 'phi2.attention.head_count': '32', 'phi2.attention.head_count_kv': '32', 'phi2.feed_forward_length': '10240', 'phi2.attention.layer_norm_epsilon': '0.000010', 'phi2.block_count': '32', 'phi2.rope.dimension_count': '32', 'general.file_type': '15'}\n",
            "Using fallback chat format: llama-2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "chain =  RetrievalQA.from_chain_type(\n",
        "    # chain='stuff',\n",
        "    retriever = retriever,\n",
        "    llm = llm,\n",
        "    return_source_documents=True\n",
        ")\n",
        "query = \"what is the page talking about?\"\n",
        "result = chain(query)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5Za6WtvEl5P",
        "outputId": "b3881268-2a0c-4260-91e4-eadfafd4182e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 44 prefix-match hit, remaining 372 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   76868.28 ms /   372 tokens (  206.64 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =   73909.25 ms /   199 runs   (  371.40 ms per token,     2.69 tokens per second)\n",
            "llama_perf_context_print:       total time =  151227.06 ms /   571 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'what is the page talking about?', 'result': '\\n\\nThis page is about the history of Wikipedia.Wikipedia is a free online encyclopedia that anyone can edit. It was created in 2001 and has grown to include articles on many different topics. The first version of Wikipedia was written in English, but now there are versions in over 100 languages. Wikipedia is run by volunteers who work together to make sure the information is accurate and up-to-date.\\n\\nWikipedia started out as a small project, with only a few people working on it. But as more and more people joined, it grew into a huge community of editors from all around the world. The first version of Wikipedia was written in English, but now there are versions in over 100 languages. This means that people who speak different languages can all contribute to Wikipedia and share their knowledge with others.\\n\\nOne of the most important things about Wikipedia is that anyone can edit it. This means that if you have something to add or change on a page, you can do it yourself! But', 'source_documents': [Document(id='c6258872-11aa-4371-80a6-651dd8ebdbed', metadata={'source': 'https://en.wikipedia.org/wiki/Natural_language_processing', 'title': 'Natural language processing - Wikipedia', 'language': 'en'}, page_content='articles needing additional referencesAll articles with unsourced statementsArticles with unsourced statements from May 2024Commons category link from Wikidata'), Document(id='caefbcf5-dbc6-4c41-be98-87ceed5f48eb', metadata={'source': 'https://en.wikipedia.org/wiki/Natural_language_processing', 'title': 'Natural language processing - Wikipedia', 'language': 'en'}, page_content='^ Winograd, Terry (1971). Procedures as a Representation for Data in a Computer Program for Understanding Natural Language (Thesis).\\n\\n^ Schank, Roger C.; Abelson, Robert P. (1977). Scripts, Plans, Goals, and Understanding: An Inquiry Into Human Knowledge Structures. Hillsdale: Erlbaum. ISBN\\xa00-470-99033-3.'), Document(id='ff15bc7d-6229-42d1-a34e-cc606e118d1b', metadata={'source': 'https://en.wikipedia.org/wiki/Natural_language_processing', 'title': 'Natural language processing - Wikipedia', 'language': 'en'}, page_content='Edit links\\n\\nArticleTalk\\n\\nEnglish\\n\\nReadEditView history\\n\\nTools\\n\\nTools\\nmove to sidebar\\nhide\\n\\nActions\\n\\nReadEditView history\\n\\nGeneral\\n\\nWhat links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\\n\\nPrint/export\\n\\nDownload as PDFPrintable version\\n\\nIn other projects\\n\\nWikimedia CommonsWikiversityWikidata item\\n\\nAppearance\\nmove to sidebar\\nhide\\n\\nFrom Wikipedia, the free encyclopedia'), Document(id='743cfb9f-11c0-4b8f-b339-c73fd6157675', metadata={'source': 'https://en.wikipedia.org/wiki/Natural_language_processing', 'title': 'Natural language processing - Wikipedia', 'language': 'en'}, page_content='Natural language processing - Wikipedia\\n\\nJump to content\\n\\nMain menu\\n\\nMain menu\\nmove to sidebar\\nhide\\n\\nNavigation\\n\\nMain pageContentsCurrent eventsRandom articleAbout WikipediaContact us\\n\\nContribute\\n\\nHelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\\n\\nSearch\\n\\nSearch\\n\\nAppearance\\n\\nDonate\\n\\nCreate account\\n\\nLog in\\n\\nPersonal tools\\n\\nDonate Create account Log in\\n\\nPages for logged out editors learn more\\n\\nContributionsTalk\\n\\nContents\\nmove to sidebar\\nhide\\n\\n(Top)\\n\\n1\\nHistory')]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "dyj-a98LKKFB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is NLP known for?\",\n",
        "    \"What are top 3 NLP techniques?\",\n",
        "    \"Who is the best author of NLP?\",\n",
        "    \"How to evaluate NLP frameworks?\",\n",
        "    \"Any latest NLP techniques?\"\n",
        "]"
      ],
      "metadata": {
        "id": "mlBFq4i1MWr1"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for ques in questions:\n",
        "    output = chain(ques)\n",
        "    results.append({\n",
        "        \"query\": ques,\n",
        "        \"answer\": output[\"result\"],\n",
        "        \"source\": \"\\n\".join([doc.page_content for doc in output[\"source_documents\"]])\n",
        "    })\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpyvbyJJMwnL",
        "outputId": "34b26ab4-73a4-4537-8f04-59767abaa809"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: 44 prefix-match hit, remaining 341 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   68492.64 ms /   341 tokens (  200.86 ms per token,     4.98 tokens per second)\n",
            "llama_perf_context_print:        eval time =   12161.69 ms /    36 runs   (  337.82 ms per token,     2.96 tokens per second)\n",
            "llama_perf_context_print:       total time =   80720.88 ms /   377 tokens\n",
            "Llama.generate: 44 prefix-match hit, remaining 441 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   88939.75 ms /   441 tokens (  201.68 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =   73476.80 ms /   199 runs   (  369.23 ms per token,     2.71 tokens per second)\n",
            "llama_perf_context_print:       total time =  162855.61 ms /   640 tokens\n",
            "Llama.generate: 44 prefix-match hit, remaining 227 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   44484.57 ms /   227 tokens (  195.97 ms per token,     5.10 tokens per second)\n",
            "llama_perf_context_print:        eval time =   38665.98 ms /   113 runs   (  342.18 ms per token,     2.92 tokens per second)\n",
            "llama_perf_context_print:       total time =   83379.62 ms /   340 tokens\n",
            "Llama.generate: 42 prefix-match hit, remaining 291 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   61668.79 ms /   291 tokens (  211.92 ms per token,     4.72 tokens per second)\n",
            "llama_perf_context_print:        eval time =   38033.04 ms /   110 runs   (  345.75 ms per token,     2.89 tokens per second)\n",
            "llama_perf_context_print:       total time =   99925.44 ms /   401 tokens\n",
            "Llama.generate: 42 prefix-match hit, remaining 344 prompt tokens to eval\n",
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   69342.87 ms /   344 tokens (  201.58 ms per token,     4.96 tokens per second)\n",
            "llama_perf_context_print:        eval time =   37891.82 ms /   112 runs   (  338.32 ms per token,     2.96 tokens per second)\n",
            "llama_perf_context_print:       total time =  107465.99 ms /   456 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'query': 'What is NLP known for?', 'answer': '\\nAI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\\n', 'source': 'Natural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.\\nCognition[edit]\\nMost higher-level NLP applications involve aspects that emulate intelligent behaviour and apparent comprehension of natural language. More broadly speaking, the technical operationalization of increasingly advanced aspects of cognitive behaviour represents one of the developmental trajectories of NLP (see trends among CoNLL shared tasks above).\\nCommon NLP tasks[edit]\\nThe following is a list of some of the most commonly researched tasks in natural language processing. Some of these tasks have direct real-world applications, while others more commonly serve as subtasks that are used to aid in solving larger tasks.\\nThough natural language processing tasks are closely intertwined, they can be subdivided into categories for convenience. A coarse division is given below.\\ncomputational psycholinguistics and cognitive neuroscience (e.g., ACT-R), however, with limited uptake in mainstream NLP (as measured by presence on major conferences[56] of the ACL). More recently, ideas of cognitive NLP have been revived as an approach to achieve explainability, e.g., under the notion of \"cognitive AI\".[57] Likewise, ideas of cognitive NLP are inherent to neural models multimodal NLP (although rarely made explicit)[58] and developments in artificial intelligence, specifically'}, {'query': 'What are top 3 NLP techniques?', 'answer': '\\n\\n<|beginofstoryusingtemplates|>\\nOnce upon a time, in the early 1990s, there was a brilliant linguist named Alice. She had dedicated her life to the study of natural language processing (NLP) and was fascinated by the different approaches used in this field. One day, she decided to delve into the history of NLP and see how it has evolved over time.\\n\\nAs she began her research, Alice discovered that there were two main approaches: symbolic and statistical. The symbolic approach involved hand-coding a set of rules for manipulating symbols and using a dictionary lookup, while the statistical approach relied on machine learning techniques such as neural networks.\\n\\nAlice was particularly intrigued by the contributions made in NLP during the 1990s. She found that the focus areas at that time included rule-based parsing, morphology, semantics, reference, and other aspects of natural language understanding. Researchers were developing tools like HPSG for computational operationalization of gener', 'source': 'Statistical NLP (1990s–present)[edit]\\nApproaches: Symbolic, statistical, neural networks[edit]\\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\\nContributionsTalk\\n\\nContents\\nmove to sidebar\\nhide\\n\\n(Top)\\n\\n1\\nHistory\\n\\nToggle History subsection\\n\\n1.1\\nSymbolic NLP (1950s – early 1990s)\\n\\n1.2\\nStatistical NLP (1990s–present)\\n\\n2\\nApproaches: Symbolic, statistical, neural networks\\n\\nToggle Approaches: Symbolic, statistical, neural networks subsection\\n\\n2.1\\nStatistical approach\\n\\n2.2\\nNeural networks\\n\\n3\\nCommon NLP tasks\\n\\nToggle Common NLP tasks subsection\\n\\n3.1\\nText and speech processing\\n\\n3.2\\nMorphological analysis\\n\\n3.3\\nSyntactic analysis\\n1980s: The 1980s and early 1990s mark the heyday of symbolic methods in NLP. Focus areas of the time included research on rule-based parsing (e.g., the development of HPSG as a computational operationalization of generative grammar), morphology (e.g., two-level morphology[5]), semantics (e.g., Lesk algorithm), reference (e.g., within Centering Theory[6]) and other areas of natural language understanding (e.g., in the Rhetorical Structure Theory). Other lines of research were continued, e.g.,'}, {'query': 'Who is the best author of NLP?', 'answer': '\\n\\nAnswer: There is no definitive answer to this question as the best author of NLP can vary depending on different perspectives and criteria. Some may argue that Noam Chomsky, a linguist who developed transformational-generative grammar, was one of the pioneers in the field of NLP. Others may point to Geoffrey Hinton, a computer scientist known for his contributions to deep learning algorithms used in NLP tasks such as machine translation and sentiment analysis. Ultimately, the best author of NLP is subjective and can be debated among experts in the field.\\n', 'source': \"Not an NLP task proper but an extension of natural language generation and other NLP tasks is the creation of full-fledged books. The first machine-generated book was created by a rule-based system in 1984 (Racter, The policeman's beard is half-constructed).[38] The first published work by a neural network was published in 2018, 1 the Road, marketed as a novel, contains sixty million words. Both these systems are basically elaborate but non-sensical (semantics-free) language models. The first\\nStatistical NLP (1990s–present)[edit]\\nHistory[edit]\\nFurther information: History of natural language processing\\nNatural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.\"}, {'query': 'How to evaluate NLP frameworks?', 'answer': '\\n\\nAnswer: There are several ways to evaluate NLP frameworks. One approach is to use a set of predefined metrics, such as BLEU or ROUGE, which compare the output of an NLP system with human-generated translations or summaries. Another approach is to conduct user studies and gather feedback from users on their experience using the NLP framework. Additionally, it can be helpful to look at the performance of the NLP system on a variety of tasks and datasets to see how well it generalizes across different contexts.\\n', 'source': '^ Guida, G.; Mauri, G. (July 1986). \"Evaluation of natural language processing systems: Issues and approaches\". Proceedings of the IEEE. 74 (7): 1026–1035. doi:10.1109/PROC.1986.13580. ISSN\\xa01558-2256. S2CID\\xa030688575.\\nExternal links[edit]\\nMedia related to Natural language processing at Wikimedia Commons\\nvteNatural language processingGeneral terms\\nAI-complete\\nBag-of-words\\nn-gram\\nBigram\\nTrigram\\nComputational linguistics\\nNatural language understanding\\nStop words\\nText processing\\nText analysis\\nArgument mining\\nCollocation extraction\\nConcept mining\\nCoreference resolution\\nDeep linguistic processing\\nDistant reading\\nInformation extraction\\nNamed-entity recognition\\nOntology learning\\nParsing\\nSemantic parsing\\nStatistical NLP (1990s–present)[edit]\\nNatural language processing (NLP) is a subfield of computer science and especially artificial intelligence. It is primarily concerned with providing computers with the ability to process data encoded in natural language and is thus closely related to information retrieval, knowledge representation and computational linguistics, a subfield of linguistics.'}, {'query': 'Any latest NLP techniques?', 'answer': '\\n\\nAnswer: Yes, there are several latest NLP techniques that have been developed. These include the use of neural networks for natural language processing tasks such as part-of-speech tagging and dependency parsing. This approach eliminates the need for intermediate tasks like these and allows for more efficient processing of text data. Additionally, the use of semantic networks and word embeddings has also greatly improved the accuracy and effectiveness of NLP techniques. These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\\n', 'source': 'Statistical NLP (1990s–present)[edit]\\nNeural networks[edit]\\nFurther information: Artificial neural network\\nA major drawback of statistical methods is that they require elaborate feature engineering. Since 2015,[22] the statistical approach has been replaced by the neural networks approach, using semantic networks[23] and word embeddings to capture semantic properties of words.\\nIntermediate tasks (e.g., part-of-speech tagging and dependency parsing) are not needed anymore.\\nApproaches: Symbolic, statistical, neural networks[edit]\\nSymbolic approach, i.e., the hand-coding of a set of rules for manipulating symbols, coupled with a dictionary lookup, was historically the first approach used both by AI in general and by NLP in particular:[18][19] such as by writing grammars or devising heuristic rules for stemming.\\nMachine learning approaches, which include both statistical and neural networks, on the other hand, have many advantages over the symbolic approach:\\nExternal links[edit]\\nMedia related to Natural language processing at Wikimedia Commons\\nvteNatural language processingGeneral terms\\nAI-complete\\nBag-of-words\\nn-gram\\nBigram\\nTrigram\\nComputational linguistics\\nNatural language understanding\\nStop words\\nText processing\\nText analysis\\nArgument mining\\nCollocation extraction\\nConcept mining\\nCoreference resolution\\nDeep linguistic processing\\nDistant reading\\nInformation extraction\\nNamed-entity recognition\\nOntology learning\\nParsing\\nSemantic parsing'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_rag_response(llm, query, answer, source):\n",
        "    eval_prompt = f\"\"\"\n",
        "    Query: {query}\n",
        "\n",
        "    Retrieved Context:\n",
        "    {source}\n",
        "\n",
        "    Model's Answer:\n",
        "    {answer}\n",
        "\n",
        "    Evaluate the answer on the following:\n",
        "    - Relevance: Is it on-topic?\n",
        "    - Faithfulness: Is it supported by the context?\n",
        "    - Completeness: Does it fully answer the query?\n",
        "\n",
        "\n",
        "    Respond in this JSON format:\n",
        "    {{\n",
        "        \"Relevance\": int,\n",
        "        \"Faithfulness\": int,\n",
        "        \"Completeness\": int,\n",
        "        \"Explanation\": str\n",
        "    }}\n",
        "    \"\"\"\n",
        "\n",
        "    return llm(eval_prompt)\n"
      ],
      "metadata": {
        "id": "nfBZS-AeKG7G"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluations = []\n",
        "for res in results:\n",
        "    grade = evaluate_rag_response(llm, res[\"query\"], res[\"answer\"], \"\\n\".join(res[\"source\"][:3]))\n",
        "    print(f\"\\n📊 Evaluation for: {res['query']}\")\n",
        "    print(grade)\n",
        "    evaluations.append({\n",
        "        \"query\": res[\"query\"],\n",
        "        \"answer\": res[\"answer\"],\n",
        "        \"grade\": grade\n",
        "    })\n",
        "print(evaluations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uE6XFEP0L6qL",
        "outputId": "44a0c0bc-caf7-40d4-9874-5382e451d914"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   36680.10 ms /   175 tokens (  209.60 ms per token,     4.77 tokens per second)\n",
            "llama_perf_context_print:        eval time =   70374.27 ms /   199 runs   (  353.64 ms per token,     2.83 tokens per second)\n",
            "llama_perf_context_print:       total time =  107493.06 ms /   374 tokens\n",
            "Llama.generate: 5 prefix-match hit, remaining 340 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation for: What is NLP known for?\n",
            "\"\"\"\n",
            "    # Your code here\n",
            "```\n",
            "\n",
            "**Solution:**\n",
            "\n",
            "```python\n",
            "def evaluate_answer(query, retrieved_context):\n",
            "    relevance = 1 if query in retrieved_context else 0\n",
            "    faithfulness = 1 if all([word.lower() in retrieved_context for word in query.split()]) else 0\n",
            "    completeness = 1 if len(retrieved_context) >= 3 and 'NLP' in retrieved_context else 0\n",
            "    explanation = \"AI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\" if relevance == 1 and faithfulness == 1 and completeness == 1 else \"\"\n",
            "\n",
            "    return {'Relevance': relevance, 'Faithfulness': faithfulness, 'Completeness': completeness, 'Explanation': explanation}\n",
            "```\n",
            "\n",
            "**Ex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   69346.71 ms /   340 tokens (  203.96 ms per token,     4.90 tokens per second)\n",
            "llama_perf_context_print:        eval time =   29439.83 ms /    86 runs   (  342.32 ms per token,     2.92 tokens per second)\n",
            "llama_perf_context_print:       total time =   98955.71 ms /   426 tokens\n",
            "Llama.generate: 4 prefix-match hit, remaining 250 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation for: What are top 3 NLP techniques?\n",
            "\n",
            "    Example:\n",
            "    {\n",
            "        \"Relevance\": 1,\n",
            "        \"Faithfulness\": 1,\n",
            "        \"Completeness\": 0,\n",
            "        \"Explanation\": \"The answer is partially relevant to the query. It mentions rule-based parsing and morphology, but it does not address the specific question about NLP techniques.\"\n",
            "    }\n",
            "\n",
            "    \n",
            "    Query: What are top \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   50607.73 ms /   250 tokens (  202.43 ms per token,     4.94 tokens per second)\n",
            "llama_perf_context_print:        eval time =    2551.95 ms /     8 runs   (  318.99 ms per token,     3.13 tokens per second)\n",
            "llama_perf_context_print:       total time =   53174.13 ms /   258 tokens\n",
            "Llama.generate: 4 prefix-match hit, remaining 245 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation for: Who is the best author of NLP?\n",
            "\"\"\"\n",
            "    # Your code here\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   50600.01 ms /   245 tokens (  206.53 ms per token,     4.84 tokens per second)\n",
            "llama_perf_context_print:        eval time =   52906.65 ms /   152 runs   (  348.07 ms per token,     2.87 tokens per second)\n",
            "llama_perf_context_print:       total time =  103829.63 ms /   397 tokens\n",
            "Llama.generate: 4 prefix-match hit, remaining 246 prompt tokens to eval\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation for: How to evaluate NLP frameworks?\n",
            "\n",
            "    The \"Relevance\" value should be 1 if the answer is relevant to the context, and 0 otherwise. The \"Faithfulness\" value should be 1 if the answer is supported by the context, and 0 otherwise. The \"Completeness\" value should be 1 if the answer fully answers the query, and 0 otherwise. The \"Explanation\" field should provide a brief explanation of why the values were assigned.\n",
            "\n",
            "    Example:\n",
            "    {\n",
            "        \"Relevance\": 1,\n",
            "        \"Faithfulness\": 1,\n",
            "        \"Completeness\": 1,\n",
            "        \"Explanation\": \"The answer is relevant to the context because it provides information about evaluating NLP frameworks.\"\n",
            "    }\n",
            "\"\"\"\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_perf_context_print:        load time =   74009.88 ms\n",
            "llama_perf_context_print: prompt eval time =   50959.99 ms /   246 tokens (  207.15 ms per token,     4.83 tokens per second)\n",
            "llama_perf_context_print:        eval time =   40341.41 ms /   113 runs   (  357.00 ms per token,     2.80 tokens per second)\n",
            "llama_perf_context_print:       total time =   91546.97 ms /   359 tokens\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📊 Evaluation for: Any latest NLP techniques?\n",
            "\"\"\"\n",
            "    # Relevance: 1 (Yes, it is on-topic)\n",
            "    # Faithfulness: 2 (The answer is supported by the context)\n",
            "    # Completeness: 3 (The answer fully answers the query)\n",
            "    explanation = \"These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\"\n",
            "    return {'Relevance': 1, 'Faithfulness': 2, 'Completeness': 3, 'Explanation': explanation}\n",
            "\n",
            "\n",
            "[{'query': 'What is NLP known for?', 'answer': '\\nAI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\\n', 'grade': '\"\"\"\\n    # Your code here\\n```\\n\\n**Solution:**\\n\\n```python\\ndef evaluate_answer(query, retrieved_context):\\n    relevance = 1 if query in retrieved_context else 0\\n    faithfulness = 1 if all([word.lower() in retrieved_context for word in query.split()]) else 0\\n    completeness = 1 if len(retrieved_context) >= 3 and \\'NLP\\' in retrieved_context else 0\\n    explanation = \"AI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\" if relevance == 1 and faithfulness == 1 and completeness == 1 else \"\"\\n\\n    return {\\'Relevance\\': relevance, \\'Faithfulness\\': faithfulness, \\'Completeness\\': completeness, \\'Explanation\\': explanation}\\n```\\n\\n**Ex'}, {'query': 'What are top 3 NLP techniques?', 'answer': '\\n\\n<|beginofstoryusingtemplates|>\\nOnce upon a time, in the early 1990s, there was a brilliant linguist named Alice. She had dedicated her life to the study of natural language processing (NLP) and was fascinated by the different approaches used in this field. One day, she decided to delve into the history of NLP and see how it has evolved over time.\\n\\nAs she began her research, Alice discovered that there were two main approaches: symbolic and statistical. The symbolic approach involved hand-coding a set of rules for manipulating symbols and using a dictionary lookup, while the statistical approach relied on machine learning techniques such as neural networks.\\n\\nAlice was particularly intrigued by the contributions made in NLP during the 1990s. She found that the focus areas at that time included rule-based parsing, morphology, semantics, reference, and other aspects of natural language understanding. Researchers were developing tools like HPSG for computational operationalization of gener', 'grade': '\\n    Example:\\n    {\\n        \"Relevance\": 1,\\n        \"Faithfulness\": 1,\\n        \"Completeness\": 0,\\n        \"Explanation\": \"The answer is partially relevant to the query. It mentions rule-based parsing and morphology, but it does not address the specific question about NLP techniques.\"\\n    }\\n\\n    \\n    Query: What are top \\n'}, {'query': 'Who is the best author of NLP?', 'answer': '\\n\\nAnswer: There is no definitive answer to this question as the best author of NLP can vary depending on different perspectives and criteria. Some may argue that Noam Chomsky, a linguist who developed transformational-generative grammar, was one of the pioneers in the field of NLP. Others may point to Geoffrey Hinton, a computer scientist known for his contributions to deep learning algorithms used in NLP tasks such as machine translation and sentiment analysis. Ultimately, the best author of NLP is subjective and can be debated among experts in the field.\\n', 'grade': '\"\"\"\\n    # Your code here\\n\\n'}, {'query': 'How to evaluate NLP frameworks?', 'answer': '\\n\\nAnswer: There are several ways to evaluate NLP frameworks. One approach is to use a set of predefined metrics, such as BLEU or ROUGE, which compare the output of an NLP system with human-generated translations or summaries. Another approach is to conduct user studies and gather feedback from users on their experience using the NLP framework. Additionally, it can be helpful to look at the performance of the NLP system on a variety of tasks and datasets to see how well it generalizes across different contexts.\\n', 'grade': '\\n    The \"Relevance\" value should be 1 if the answer is relevant to the context, and 0 otherwise. The \"Faithfulness\" value should be 1 if the answer is supported by the context, and 0 otherwise. The \"Completeness\" value should be 1 if the answer fully answers the query, and 0 otherwise. The \"Explanation\" field should provide a brief explanation of why the values were assigned.\\n\\n    Example:\\n    {\\n        \"Relevance\": 1,\\n        \"Faithfulness\": 1,\\n        \"Completeness\": 1,\\n        \"Explanation\": \"The answer is relevant to the context because it provides information about evaluating NLP frameworks.\"\\n    }\\n\"\"\"\\n'}, {'query': 'Any latest NLP techniques?', 'answer': '\\n\\nAnswer: Yes, there are several latest NLP techniques that have been developed. These include the use of neural networks for natural language processing tasks such as part-of-speech tagging and dependency parsing. This approach eliminates the need for intermediate tasks like these and allows for more efficient processing of text data. Additionally, the use of semantic networks and word embeddings has also greatly improved the accuracy and effectiveness of NLP techniques. These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\\n', 'grade': '\"\"\"\\n    # Relevance: 1 (Yes, it is on-topic)\\n    # Faithfulness: 2 (The answer is supported by the context)\\n    # Completeness: 3 (The answer fully answers the query)\\n    explanation = \"These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\"\\n    return {\\'Relevance\\': 1, \\'Faithfulness\\': 2, \\'Completeness\\': 3, \\'Explanation\\': explanation}\\n\\n'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "[print(eval) for eval in evaluations]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orwxFHdWZITB",
        "outputId": "12ef1fbc-f043-4e68-9002-bdd9e53eba26"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'query': 'What is NLP known for?', 'answer': '\\nAI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\\n', 'grade': '\"\"\"\\n    # Your code here\\n```\\n\\n**Solution:**\\n\\n```python\\ndef evaluate_answer(query, retrieved_context):\\n    relevance = 1 if query in retrieved_context else 0\\n    faithfulness = 1 if all([word.lower() in retrieved_context for word in query.split()]) else 0\\n    completeness = 1 if len(retrieved_context) >= 3 and \\'NLP\\' in retrieved_context else 0\\n    explanation = \"AI is known for its ability to process data and understand natural language, making it a valuable tool in fields such as information retrieval, knowledge representation, and computational linguistics.\" if relevance == 1 and faithfulness == 1 and completeness == 1 else \"\"\\n\\n    return {\\'Relevance\\': relevance, \\'Faithfulness\\': faithfulness, \\'Completeness\\': completeness, \\'Explanation\\': explanation}\\n```\\n\\n**Ex'}\n",
            "{'query': 'What are top 3 NLP techniques?', 'answer': '\\n\\n<|beginofstoryusingtemplates|>\\nOnce upon a time, in the early 1990s, there was a brilliant linguist named Alice. She had dedicated her life to the study of natural language processing (NLP) and was fascinated by the different approaches used in this field. One day, she decided to delve into the history of NLP and see how it has evolved over time.\\n\\nAs she began her research, Alice discovered that there were two main approaches: symbolic and statistical. The symbolic approach involved hand-coding a set of rules for manipulating symbols and using a dictionary lookup, while the statistical approach relied on machine learning techniques such as neural networks.\\n\\nAlice was particularly intrigued by the contributions made in NLP during the 1990s. She found that the focus areas at that time included rule-based parsing, morphology, semantics, reference, and other aspects of natural language understanding. Researchers were developing tools like HPSG for computational operationalization of gener', 'grade': '\\n    Example:\\n    {\\n        \"Relevance\": 1,\\n        \"Faithfulness\": 1,\\n        \"Completeness\": 0,\\n        \"Explanation\": \"The answer is partially relevant to the query. It mentions rule-based parsing and morphology, but it does not address the specific question about NLP techniques.\"\\n    }\\n\\n    \\n    Query: What are top \\n'}\n",
            "{'query': 'Who is the best author of NLP?', 'answer': '\\n\\nAnswer: There is no definitive answer to this question as the best author of NLP can vary depending on different perspectives and criteria. Some may argue that Noam Chomsky, a linguist who developed transformational-generative grammar, was one of the pioneers in the field of NLP. Others may point to Geoffrey Hinton, a computer scientist known for his contributions to deep learning algorithms used in NLP tasks such as machine translation and sentiment analysis. Ultimately, the best author of NLP is subjective and can be debated among experts in the field.\\n', 'grade': '\"\"\"\\n    # Your code here\\n\\n'}\n",
            "{'query': 'How to evaluate NLP frameworks?', 'answer': '\\n\\nAnswer: There are several ways to evaluate NLP frameworks. One approach is to use a set of predefined metrics, such as BLEU or ROUGE, which compare the output of an NLP system with human-generated translations or summaries. Another approach is to conduct user studies and gather feedback from users on their experience using the NLP framework. Additionally, it can be helpful to look at the performance of the NLP system on a variety of tasks and datasets to see how well it generalizes across different contexts.\\n', 'grade': '\\n    The \"Relevance\" value should be 1 if the answer is relevant to the context, and 0 otherwise. The \"Faithfulness\" value should be 1 if the answer is supported by the context, and 0 otherwise. The \"Completeness\" value should be 1 if the answer fully answers the query, and 0 otherwise. The \"Explanation\" field should provide a brief explanation of why the values were assigned.\\n\\n    Example:\\n    {\\n        \"Relevance\": 1,\\n        \"Faithfulness\": 1,\\n        \"Completeness\": 1,\\n        \"Explanation\": \"The answer is relevant to the context because it provides information about evaluating NLP frameworks.\"\\n    }\\n\"\"\"\\n'}\n",
            "{'query': 'Any latest NLP techniques?', 'answer': '\\n\\nAnswer: Yes, there are several latest NLP techniques that have been developed. These include the use of neural networks for natural language processing tasks such as part-of-speech tagging and dependency parsing. This approach eliminates the need for intermediate tasks like these and allows for more efficient processing of text data. Additionally, the use of semantic networks and word embeddings has also greatly improved the accuracy and effectiveness of NLP techniques. These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\\n', 'grade': '\"\"\"\\n    # Relevance: 1 (Yes, it is on-topic)\\n    # Faithfulness: 2 (The answer is supported by the context)\\n    # Completeness: 3 (The answer fully answers the query)\\n    explanation = \"These advancements have been made possible by the development of artificial intelligence and machine learning algorithms that can understand and manipulate natural language.\"\\n    return {\\'Relevance\\': 1, \\'Faithfulness\\': 2, \\'Completeness\\': 3, \\'Explanation\\': explanation}\\n\\n'}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None, None]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAGAS"
      ],
      "metadata": {
        "id": "vsfRUecvZOdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ragas.metrics import faithfulness, answer_relevancy, context_precision\n",
        "from ragas.evaluation import evaluate\n",
        "\n",
        "# Pick any subset of metrics\n",
        "results = evaluate(\n",
        "    dataset,\n",
        "    metrics=[faithfulness, answer_relevancy, context_precision]\n",
        ")\n",
        "print(\"📈 Evaluation Results:\")\n",
        "for metric, score in results.items():\n",
        "    print(f\"{metric.name}: {round(score, 3)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "nAkVCIiQZMn0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}